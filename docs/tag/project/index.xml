<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>project | Gregor von Laszewski</title>
    <link>/tag/project/</link>
      <atom:link href="/tag/project/index.xml" rel="self" type="application/rss+xml" />
    <description>project</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 01 Aug 2021 01:33:02 -0500</lastBuildDate>
    <image>
      <url>/images/icon_hufdd866d90d76849587aac6fbf27da1ac_464_512x512_fill_lanczos_center_2.png</url>
      <title>project</title>
      <link>/tag/project/</link>
    </image>
    
    <item>
      <title>Kubernetes Raspberry Pi Cluster for Data Science (largely completed)</title>
      <link>/post/project-pi-kubernetes/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-pi-kubernetes/</guid>
      <description>&lt;p&gt;Be part of a team to create a kubernetes cluster on a Raspberry pi.&lt;/p&gt;
&lt;p&gt;At &lt;a href=&#34;http://piplanet.org&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster with a convenient burning approach of SD Cards. The Pis will
receive preconfigured SD Cards that allow starting a cluster when the
Pis are switched on. You will be able to communicate from the COmputer
on which you burned the Pis and each of the PIs. You will also be able
to communicate between the PIs.&lt;/p&gt;
&lt;p&gt;Now that the basic cluster is set up, we like to install a distributed
quieing system for the cluster based on Kubernetes. Kubernetes is a very popular
farmework for container clusters.&lt;/p&gt;
&lt;p&gt;We like to implement a simple one line deployment comamndline tool supported by an API that deploys kubernetes on a list of hosts specified by the ip adress. This includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to have financial resources to buy yourself the material for creating a PI cluster. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at &lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you also have a Laptop or desktop to which you like to connect the PI, make sure it can run docker (Windows Home will not work). However, one of the Raspberry PIs will do.&lt;/li&gt;
&lt;li&gt;Significant Python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;We like you to explore the deployment both on RaspberryOS and on Ubuntu.&lt;/p&gt;
&lt;p&gt;Please be aware that the goal is not to replicate tutorials from the Web that require input by hand or repeated installs on the various PIs. Instead, we like to have  a single command such as&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cms pi cluster deploy kubernetes --hosts &amp;quot;red,red[01-red02]&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where simple options such as the hostnames in the pi cluster are used.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all projects on the internet that do something similar&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Explore the old code and identify what is wrong with it (just by looking at it)&lt;/li&gt;
&lt;li&gt;Identify a base method. This can use existing DevOps approaches such as ansible, cheff, snapcraft (ubuntu), or cloudmesh parallel runtime methods. However, the use will be hidden through an API and command line tool. Evaluate the llnl simple deployment method which can be used. Also exploer if there is a snap available.&lt;/li&gt;
&lt;li&gt;From the command line, derive some API interface and use FastAPI to
implement it.&lt;/li&gt;
&lt;li&gt;Identify a mechanism on how to deal with the security.&lt;/li&gt;
&lt;li&gt;Before you start implementing, describe and showcase a couple of commands on how to use it.&lt;/li&gt;
&lt;li&gt;Your deployment should target Ubuntu on the PI as well as RaspberryOS. If there is only time for one, pick one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Map-Reduce Raspberry Pi Cluster as a Local Cloud</title>
      <link>/post/project-pi-spark/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-pi-spark/</guid>
      <description>&lt;p&gt;Be part of a team to create a spark cluster on a Raspberry pi.&lt;/p&gt;
&lt;p&gt;At &lt;a href=&#34;http://piplanet.org&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster with a convenient burning approach of SD Cards. The Pis will
receive preconfigured SD Cards that allow starting a cluster when the
Pis are switched on. You will be able to communicate from the COmputer
on which you burned the Pis and each of the PIs. You will also be able
to communicate between the PIs.&lt;/p&gt;
&lt;p&gt;Now that the basic cluster is set up, we like to install a data science
framework such as Spark and Hadoop and showcase its use. Although
previously we had some effort implementing Hadoop and Spark, these
efforts were not production-ready and had significant gaps in its
implementation.&lt;/p&gt;
&lt;p&gt;We like to reimplement these projects and significantly improve them by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to have financial resources to buy yourself the material for creating a PI cluster. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at &lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you also have a Laptop or desktop to which you like to connect the PI, make sure it can run docker (Windows Home will not work). However, one of the Raspberry PIs will do.&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;There is a great overlap between installing Hadoop and Spark on a
cluster. We recommend that the team slits up the work but collaborates
intensely to create a uniform solution.&lt;/p&gt;
&lt;p&gt;Please be aware that the goal is not to replicate tutorials from the Web that require input by hand or repeated installs on the various PIs. Instead, we like to have  a single command such as&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cms pi cluster deploy spark --hosts &amp;quot;red,red[01-red02]&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where simple options such as the hostnames in the pi cluster are used.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all projects on the internet that do something similar&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Explore the old code and identify what is wrong with it (just by looking at it)&lt;/li&gt;
&lt;li&gt;Identify a new deployment method. This can use existing DevOps approaches such as ansible, cheff, snapcraft (ubuntu), or cloudmesh parallel runtime methods. However, the use will be hidden through an API and command line tool.&lt;/li&gt;
&lt;li&gt;From the command line, derive some API interface and use FastAPI to
implement it.&lt;/li&gt;
&lt;li&gt;Identify a mechanism on how to deal with the security.&lt;/li&gt;
&lt;li&gt;Before you start implementing, describe and showcase a couple of commands on how to use it.&lt;/li&gt;
&lt;li&gt;Your deployment should target Ubuntu on the PI as well as RaspberryOS. If there is only time for one, pick one.
Note that Hadoop or Spark may also be able to be installed on the PIs with snapcraft. Discuss how this solution could be used for the implementation of this project.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Cloud NLP Service</title>
      <link>/post/project-nlp/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-nlp/</guid>
      <description>&lt;p&gt;Be part of a team to create a multicloud natural language processing service.&lt;/p&gt;
&lt;p&gt;Your goal will be to develop an API, secure REST, and command line
tool that easily interfaces with natural language services of multiple
cloud providers.  Your integrated service will utilize all or one
of them to achieve a task related to NLP analysis.&lt;/p&gt;
&lt;p&gt;This is especially useful for data scientists that may want to access
multiple cloud providers and eliminate vendor lock-in or to access
services that other providers do not offer.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;p&gt;You will be developing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A comparison of NLP cloud services.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aylien&lt;/li&gt;
&lt;li&gt;Text Summarization&lt;/li&gt;
&lt;li&gt;Twinword Text Analysis&lt;/li&gt;
&lt;li&gt;IBM Watson Alchemy&lt;/li&gt;
&lt;li&gt;RxNLP&lt;/li&gt;
&lt;li&gt;Linguakit&lt;/li&gt;
&lt;li&gt;Geneea Interpretor NLP&lt;/li&gt;
&lt;li&gt;MLP CLoud&lt;/li&gt;
&lt;li&gt;Natural Language AI &lt;a href=&#34;https://cloud.google.com/natural-language&#34;&gt;https://cloud.google.com/natural-language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CLoud Factory NLP &lt;a href=&#34;https://www.cloudfactory.com/services/nlp&#34;&gt;https://www.cloudfactory.com/services/nlp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MonkeyLearn&lt;/li&gt;
&lt;li&gt;MeaningCloud&lt;/li&gt;
&lt;li&gt;Lexalytics&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;li&gt;There could be many more&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Distinguish them by characteristics, create a table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A command line interface to the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Rest API that calls out other services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Python API that wraps several services into a convenient library&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A manual describing the functionality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Put everything in a container so it can be run on Linux, Mac and
Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a convenient command line tool that allows starting the service, interacting with it, and making this really easy to use. THe
command line will hide the docker commands while providing human
readable abbreviations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver unit tests with pytests&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver a high-quality report including benchmarks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrate authentication to the cloud providers and to the REST service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Yaml for the configuration of the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the development in a container using 20.04. We will create a DOckerfile&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code will be developed in GitHub at cloudmesh-nlp, which will be set up by Gregor&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A computer on which you can run docker (Windows Home will not work)&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;Showcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;Be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;start with a Survey&lt;/li&gt;
&lt;li&gt;design the command line interface first as that may be the easiest and will showcase how to design the API&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Cloud Virtual Directory</title>
      <link>/post/project-vdir/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-vdir/</guid>
      <description>&lt;h2 id=&#34;virtual-directory&#34;&gt;Virtual directory&lt;/h2&gt;
&lt;p&gt;Be part of a team to create a multicloud virtual directory.&lt;/p&gt;
&lt;p&gt;Your goal will be to to develop an API, secure REST, and commandline
tool that easily interfaces with storage servoces such as file based
services and object stores osted on the internet to create a virtual
directory that accesses the files by directory name and filename.&lt;/p&gt;
&lt;p&gt;This is espaciellyusefull for data scientists that may want to strore
their data on multiple cloud providers and eliminate vendor lockin.&lt;/p&gt;
&lt;p&gt;It could also be useful for geographically distributed files that
allow services to utilize them based on speed of access or the
physical location.&lt;/p&gt;
&lt;p&gt;In a previous version of cloudmesh we have developed an integrated
approach for compute and storage services. However in this project we
like to explore some new features and separate the implementation form
the main cloudmesh.&lt;/p&gt;
&lt;p&gt;In the past cloudmesh has conducted two such projects&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/manual/storage.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/manual/storage.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/manual/vdir.html&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/manual/vdir.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We like to reimplement thise projects and significantly improve them by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Adding more providers&lt;/li&gt;
&lt;li&gt;Adding a secure rest service for acceassing the virdual directory&lt;/li&gt;
&lt;li&gt;Creating a slick user interface to brows the virdula directry&lt;/li&gt;
&lt;li&gt;Put everything in a container so it can be run on Linux, MAc and
Windows.&lt;/li&gt;
&lt;li&gt;Dreate a convenient commandline tool that allows starting of the
service, interacting with it and make this real easy to use. THe
commandline will hide the docker commands while providing human
readable abbreviations.&lt;/li&gt;
&lt;li&gt;Deliver unit tests with pytests&lt;/li&gt;
&lt;li&gt;Deliver a high quality report including benchmarks&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A computer on which you can run docker (Windows Home will not work)&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgable with github (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;One of the two projects used a single file taansfer logic, while the
other used a parallel file transfer option. It will be up to you to decide how to proceed.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In the first week collect all clouds and services that could be
used in this project. THis work should be split between the
teammembers and a detailed analysis including features including
authentication and security must be conducted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You will need to evaluate the previous efforts and decide how to proceed.&lt;/p&gt;
&lt;p&gt;One way is to select several cloudas and define an abstraction that
has to be implemented for each cloud. this can be actually bes
demonstrated by an example commandline such as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vdir mkdir DIR
vdir cd [DIR]
vdir ls [DIR]
vdir add [FILEENDPOINT] [DIR_AND_NAME]
vdir delete [DIR_OR_NAME]
vdir status [DIR_OR_NAME]
vdir get NAME DESTINATION
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the commandline derive some API interface and use FastAPI to
implement it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identify a mechnism on how to deal with the security. AN encrypted
file with your cloud credentials will be sufficient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the limitations of the previous vdir project is that the
fill add function is done only on a single file but not on a
recursive file tree. Naturally we need to upload, single files,
multiple files and filetrees, similar to the unix command rsync.
This has been implemented in the storage command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;storage [--storage=SERVICE] [--parallel=N] create dir DIRECTORY
storage [--storage=SERVICE] [--parallel=N] get SOURCE DESTINATION [--recursive]
storage [--storage=SERVICE] [--parallel=N] put SOURCE DESTINATION [--recursive]
storage [--storage=SERVICE] [--parallel=N] list [SOURCE] [--recursive] [--output=OUTPUT]
storage [--storage=SERVICE] [--parallel=N] delete SOURCE
storage [--storage=SERVICE] search  DIRECTORY FILENAME [--recursive] [--output=OUTPUT]
storage [--storage=SERVICE] sync SOURCE DESTINATION [--name=NAME] [--async]
storage [--storage=SERVICE] sync status [--name=NAME]
storage config list [--output=OUTPUT]
storage [--parallel=N] copy SOURCE DESTINATION [--recursive]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We like naturally to (re-)implement the storage command.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before you start implementing describe and showcase a couple of commands on how to used it.
Our command will be called cdir (for cloudmesh dir so it is easy to distinguish)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure that you provide a comprehensive list of potentail cloud
storage ervices. Distinguish between for pay and free services.&lt;/p&gt;
&lt;p&gt;The list could include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWS Drive&lt;/li&gt;
&lt;li&gt;AWS Object Store&lt;/li&gt;
&lt;li&gt;An ssh accesible computer&lt;/li&gt;
&lt;li&gt;Azure&lt;/li&gt;
&lt;li&gt;Box&lt;/li&gt;
&lt;li&gt;Degoo&lt;/li&gt;
&lt;li&gt;Dropbox&lt;/li&gt;
&lt;li&gt;Google Drive&lt;/li&gt;
&lt;li&gt;Google docs/drive&lt;/li&gt;
&lt;li&gt;Icedrive&lt;/li&gt;
&lt;li&gt;MEGA&lt;/li&gt;
&lt;li&gt;MediaFire&lt;/li&gt;
&lt;li&gt;MediaFire&lt;/li&gt;
&lt;li&gt;Mega&lt;/li&gt;
&lt;li&gt;Nextcloud&lt;/li&gt;
&lt;li&gt;OneCloud&lt;/li&gt;
&lt;li&gt;OneDrive&lt;/li&gt;
&lt;li&gt;Oracle&lt;/li&gt;
&lt;li&gt;Sync.com&lt;/li&gt;
&lt;li&gt;WebDAV&lt;/li&gt;
&lt;li&gt;Your local computer&lt;/li&gt;
&lt;li&gt;iCloud&lt;/li&gt;
&lt;li&gt;iDrive&lt;/li&gt;
&lt;li&gt;idrive&lt;/li&gt;
&lt;li&gt;pCloud&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Are there companies or services that already offer this?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When defining the REST API we like to be able to use a dirname and
a basename within the api similar to python. to deal with provider
specific filenames we do have two urls for a file.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{provider}/{dirname}/{basename}&lt;/code&gt;
&lt;code&gt;{dirname}/{basename}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The first allows duplication of files between services, the latter
defines a &amp;ldquo;preferred&amp;rdquo; url based on some criteria. It could be that
we just set the preferred provider for the file.&lt;/p&gt;
&lt;p&gt;Note that the introduction of duplicating files is new&lt;/p&gt;
&lt;p&gt;It also requires an update so that the file with the newest
timestamp gets updated on all registered serrvices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Please note that the project addresses the ability to integrate
object store and regular file system based storage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not all implementation must be done on Ubuntu20.04 which is run
via a container. The graphical component is done wia moder Web
view technologies. Wile all other implementation can be done in
python, the GUI can also be implemented in JavaScript. The service
accessing other services must be properly protected which is easy
as we assum it runs on localhost and we can appropriately secure
it with common solutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of qeuestions, pleas econtact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Cloud X Service</title>
      <link>/post/project-multicloud-x/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-multicloud-x/</guid>
      <description>&lt;p&gt;Be part of a team to create a multicloud access to a service offered
by multiple cloud or cloud service providers.&lt;/p&gt;
&lt;p&gt;Your goal will be to develop an API, secure REST, and command line
tool that easily interfaces with a service framework or tool offered by multiple
cloud providers.  Your integrated service will utilize all or one
of them to achieve a task related to an analysis conducted by X.&lt;/p&gt;
&lt;p&gt;This is especially useful for data scientists that may want to access
multiple cloud providers and eliminate vendor lock-in or to access
services that other providers do not offer.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;p&gt;You will be developing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A comparison of X on various cloud or cloud service providers.
Distinguish them by chracteroistics, create a table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A command line interface to the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Rest API that calls out other services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Python API that wraps several services into a convenient library&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A manual describing the functionality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Put everything in a container so it can be run on Linux, Mac and
Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a convenient command line tool that allows starting the service, interacting with it, and making this easy to use. The command line will hide the docker commands while providing human
readable abbreviations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver unit tests with pytests&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver a high-quality report including benchmarks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrate authentication to the cloud providers and to the REST service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Yaml for the configuration of the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the development in a container using 20.04. We will create a DOckerfile&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code will be developed in GitHub at cloudmesh, which will be set up by Gregor&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A computer on which you can run docker (Windows Home will not work)&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;Showcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;Be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;start with a Survey&lt;/li&gt;
&lt;li&gt;design the command line interface first as that may be the easiest and will showcase how to design the API&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster for Home Cloud Services</title>
      <link>/post/project-homecloud/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-homecloud/</guid>
      <description>&lt;p&gt;Be part of a team to create a home clouds on  Raspberry Pi&amp;rsquo;s.&lt;/p&gt;
&lt;p&gt;This project is a mini project and may require additional activities to qualify for a long term project. Nevertheless, it can be made suitable due to comparision and integration with enough activities. We explain why it is a miniproject and how you can make it a full project. Thos project only requires the use of a single Raspberry Pi 4 with 8GB of storage.&lt;/p&gt;
&lt;p&gt;At &lt;a href=&#34;http://piplanet.org&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster. Instead of burning a cluster you will use the burn command to burn a single PI 4.
Now that the basic cluster is set up, we like you to explore any available &amp;ldquo;home cloud&amp;rdquo; setup on your PI and document it. IN addition, you will implement a simple one line deployment comamndline tool supported by an API that deploys slurm on a list of hosts specified by the ip adress. This includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You need to have financial resources to buy yourself the material for creating a single PI. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at &lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition we will explore the integration of at least one additinal PI. This could be another PI or a PI Zero with WiFi. You hav two options here. A use the Pi Zero and hoock it up to a battery so you can move the PI to record an attached sensor such as temperature, humidity, preasure and record the values for it when moving around. Another thing would be to measuer the strenght of the Wifi network and other networks you may discover. Populate the information to your manager PI and create a Web secure page that reports the results that you then observe from another computer. Furthermore, identify at least one home automation hardware that you can connect to the PI, such as Alexa or Google switches, Garage door openers (commercial) and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Significant Python knowledge&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Be highly motivated&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Be willing to have meetings on this project once or twice a week&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SHowcase significant progress over the lifetime of the project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Conduct task management in GitHub (Gregor will explain)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Be honest and not hiding problems or implementation bugs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;There is a great deal of flexibility in this project and we like that you explore your ideas but work with Gregor to identify if they are feasable and complex enough. Gregor has a &amp;ldquo;secret&amp;rdquo; PI notebook that can be used to interface with the sensors and is easy to use. However, we do not have yet anything about interfacing to commecrcial products.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all Google like service and other home products that could be integrated in this project.&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Find out how to burn a single PI.&lt;/li&gt;
&lt;li&gt;Try out and &lt;strong&gt;document&lt;/strong&gt; the various cloud services. Evaluate if they work and how to use elementary featuers of them&lt;/li&gt;
&lt;li&gt;Identify commercial hardware that can be accessed from the PI and showcase how to use it.&lt;/li&gt;
&lt;li&gt;Develop a FastAPI based secure REST service that uses such hardware (such as alexa switches). Showcase and document how to access them from Python, but also how to access them from your secure REST service. Develop a commandline interface for it (Gregor will help setting this commandline interface up)&lt;/li&gt;
&lt;li&gt;Communicate regularly with Gregor on ideas that you have&lt;/li&gt;
&lt;li&gt;An absolute must is the development of the documentation. THis project may have more documentation needs than others&lt;/li&gt;
&lt;li&gt;Optional: Discuss the integration of mqtt. Showcase&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slurm Raspberry Pi Cluster for Data Science</title>
      <link>/post/project-pi-slurm/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-pi-slurm/</guid>
      <description>&lt;p&gt;Be part of a team to create a slurm cluster on a Raspberry pi.&lt;/p&gt;
&lt;p&gt;At &lt;a href=&#34;http://piplanet.org&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster with a convenient burning approach of SD Cards. The Pis will
receive preconfigured SD Cards that allow starting a cluster when the
Pis are switched on. You will be able to communicate from the COmputer
on which you burned the Pis and each of the PIs. You will also be able
to communicate between the PIs.&lt;/p&gt;
&lt;p&gt;Now that the basic cluster is set up, we like to install a distributed
quieing system for the cluster based on Slurm. Slurm is a very popular
farmework for HPC computers, but also allows scheduling of long
running jobs for data science.&lt;/p&gt;
&lt;p&gt;We like to implement a simple one line deployment comamndline tool supported by an API that deploys slurm on a list of hosts specified by the ip adress. This includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to have financial resources to buy yourself the material for creating a PI cluster. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at &lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you also have a Laptop or desktop to which you like to connect the PI, make sure it can run docker (Windows Home will not work). However, one of the Raspberry PIs will do.&lt;/li&gt;
&lt;li&gt;Significant Python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;We like you to explore the deployment both on RaspberryOS and on Ubuntu.&lt;/p&gt;
&lt;p&gt;Please be aware that the goal is not to replicate tutorials from the Web that require input by hand or repeated installs on the various PIs. Instead, we like to have  a single command such as&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cms pi cluster deploy slurm --hosts &amp;quot;red,red[01-red02]&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where simple options such as the hostnames in the pi cluster are used.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all projects on the internet that do something similar&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Explore the old code and identify what is wrong with it (just by looking at it)&lt;/li&gt;
&lt;li&gt;Identify a base method. This can use existing DevOps approaches such as ansible, cheff, snapcraft (ubuntu), or cloudmesh parallel runtime methods. However, the use will be hidden through an API and command line tool. Evaluate the llnl simple deployment method which can be used. Also exploer if there is a snap available.&lt;/li&gt;
&lt;li&gt;From the command line, derive some API interface and use FastAPI to
implement it.&lt;/li&gt;
&lt;li&gt;Identify a mechanism on how to deal with the security.&lt;/li&gt;
&lt;li&gt;Before you start implementing, describe and showcase a couple of commands on how to use it.&lt;/li&gt;
&lt;li&gt;Your deployment should target Ubuntu on the PI as well as RaspberryOS. If there is only time for one, pick one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bookmanager</title>
      <link>/project/bookmanager/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 -0400</pubDate>
      <guid>/project/bookmanager/</guid>
      <description>&lt;h1 id=&#34;bookmanager&#34;&gt;Bookmanager&lt;/h1&gt;
&lt;p&gt;Bookmanager is a tool to create a publication from a number of sources on the
internet. It is especially useful to create customized books, lecture notes, or
handouts. Content is best integrated in markdown format as it is very fast to
produce the output. At present we only produce epubs, but it will be easy to
also create pdf, html, work, odt and others. As we use pandoc we can support the
formats supported by it.&lt;/p&gt;
&lt;p&gt;Implemented Features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Table of contents with indentation levels can be specified via yaml&lt;/li&gt;
&lt;li&gt;Special variable substitution of elements defined in the yaml file&lt;/li&gt;
&lt;li&gt;Documents are fetched from github&lt;/li&gt;
&lt;li&gt;The documents will be inspected and the images found in them are fetched
(we assume the images are relative to the document, http links will not be modified)&lt;/li&gt;
&lt;li&gt;Automatic generation of a cover page&lt;/li&gt;
&lt;li&gt;Output is generated in a dest directory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Planed enhancements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;integration of References via pandoc citeref&lt;/li&gt;
&lt;li&gt;integration of Section, Table, Image references via pandoc crossref&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you like to help get in contact with Gregor von Laszewski
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install cyberaide-bookmanager
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;bookmanager -- a helper to create books from markdown files in a yaml TOC.

Usage:
  bookmanager version
  bookmanager YAML cover
  bookmanager YAML get [--format=FORMAT] [--force]
  bookmanager YAML download
  bookmanager YAML level
  bookmanager YAML epub [--force]
  bookmanager YAML pdf
  bookmanager YAML html
  bookmanager YAML docx
  bookmanager YAML check [--format=FORMAT]
  bookmanager YAML urls [--format=FORMAT]
  bookmanager YAML list [--format=FORMAT] [--details]

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cloudmesh JavaScript User Interface</title>
      <link>/project/cloudmesh-gui/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 -0400</pubDate>
      <guid>/project/cloudmesh-gui/</guid>
      <description>&lt;p&gt;Please contact me if you like to help developing a JavaScript Graphical user interface&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Cloud Mangement with Cloudmesh</title>
      <link>/project/cloudmesh/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 -0400</pubDate>
      <guid>/project/cloudmesh/</guid>
      <description>&lt;h1 id=&#34;about&#34;&gt;About&lt;/h1&gt;
&lt;p&gt;Cloudmesh enables you to access multi-cloud environments such as AWS,
Azure, Google, and OpenStack Cloudsvery easily. To start a vm on AWS you
can say&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms set cloud=AWS cms vm boot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To start one on Azure, simply set the cloud accordingly&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cms set cloud=AWS cms vm boot 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://cloudmesh-community.github.io/cm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloudmesh&lt;/a&gt; is an evolution
of our previous tool that has been used by hundreds of students and
cloud practitioners to interact easily with clouds to create a service
mashup to access common cloud services across a number of cloud
providers.&lt;/p&gt;
&lt;p&gt;It is under active development and managed in github at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Documentation: &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-manual/&#34;&gt;https://github.com/cloudmesh/cloudmesh-manual/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href=&#34;https://github.com/cloudmesh/&#34;&gt;https://github.com/cloudmesh/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It has a variety of repositories that add features to cloudmesh based on
needs by the user.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Library&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-common&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Common&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Simplifies system, console, and argument management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-cmd5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shell&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Dophisticated command shell and line interpreter with plugins&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-installer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Installer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Convenient source code installer and manager for developers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-cloud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Database&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A database for caching cloud interactions (based on MongoDB)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-cloud&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Compute&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Cloud Compute Providers for AWS, Azure, Google, Openstack&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-storage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Storage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Cloud Storage Providers for AWS, Azure, Google, Openstack&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-workflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Workflow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Plugin for managing workflows&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-emr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Map/Reduce&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Plugin for managing AWS Elastic MapReduce (EMR)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenApi&lt;/td&gt;
&lt;td&gt;OpenAPI based REST service interfaces&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is under active development and managed in github at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Documentation: &lt;a href=&#34;https://github.com/cloudmesh/cloudmesh-manual/&#34;&gt;https://github.com/cloudmesh/cloudmesh-manual/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href=&#34;https://github.com/cloudmesh/&#34;&gt;https://github.com/cloudmesh/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NIST Big Data</title>
      <link>/project/cloudmesh-nist/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 -0400</pubDate>
      <guid>/project/cloudmesh-nist/</guid>
      <description>&lt;h2 id=&#34;nist-big-data-refernce-architecture&#34;&gt;NIST Big Data Refernce Architecture&lt;/h2&gt;
&lt;p&gt;Reference architectures provide &lt;em&gt;an authoritative source of information
about a specific subject area that guides and constrains the
instantiations of multiple architectures and solutions&lt;/em&gt;. Reference
architectures generally serve as a foundation for solution architectures
and may also be used for comparison and alignment of instantiations of
architectures and solutions. The goal of the NBD-PWG Reference
Architecture Subgroup is to develop an open reference architecture for
Big Data that achieves the following objectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides a common language for the various stakeholders;&lt;/li&gt;
&lt;li&gt;Encourages adherence to common standards, specifications, and patterns;&lt;/li&gt;
&lt;li&gt;Provides consistent methods for implementation of technology to solve
similar problem sets;&lt;/li&gt;
&lt;li&gt;Illustrates and improves understanding of the various Big Data
components, processes, and systems, in the context of a vendor- and
technology-agnostic Big Data conceptual model;&lt;/li&gt;
&lt;li&gt;Provides a technical reference for U.S. government departments,
agencies, and other consumers to understand, discuss, categorize, and
compare Big Data solutions; and&lt;/li&gt;
&lt;li&gt;Facilitates analysis of candidate standards for interoperability,
portability, reusability, and extendibility.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/publication/las-19-nist&#34;&gt;NIST Big Data Interoperability Framework: Volume 8, Reference
Architecture Interfaces Wo. L Chang Gregor von Laszewski Jun 1,
2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Raspberry Pi Cloud Cluster</title>
      <link>/project/pi-cloud-cluster/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 -0400</pubDate>
      <guid>/project/pi-cloud-cluster/</guid>
      <description>&lt;h2 id=&#34;raspberry-pi-cloud-cluster&#34;&gt;Raspberry Pi Cloud Cluster&lt;/h2&gt;
&lt;p&gt;THis project is open to anyone wanting to work with me on building a
Raspberry Pi Cloud Cluster. We have 100-200 Raspberry Pis which provide
an ideal playground for building your own cluster and explore
distributed computing algorithms on real hardware. In this project you
would be building a cluster and than putting cloud software on the
cluster to make it our own cloud service.&lt;/p&gt;
&lt;p&gt;Such software can include but is not limited to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Kubernetwes&lt;/li&gt;
&lt;li&gt;Map/Reduce&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When chosing this project one must address the issue of dealing with
many compute resources at a time. THis can be done with PXE boot or with
our cm-burn.&lt;/p&gt;
&lt;p&gt;Students that chose this project will have an office space in the
building that I sit. In addition each student will be given a 5 node
cluster thay can work at home on (non smoking environment) that must be
returned after the project is completed.&lt;/p&gt;
&lt;h3 id=&#34;cm-burn&#34;&gt;cm-burn&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;cm-burn&lt;/code&gt; is a program to burn many SD cards for the preparation of
building clusters with Raspberry Pi&amp;rsquo;s.  The program is developed in
Python and is portable on Linux, Windows, and OSX. It allows users to
create readily bootable SD cards that have the network configured,
contain a public ssh key from your machine that you used to configure
the cards.  The unique feature is that you can burn multiple cards in
a row.&lt;/p&gt;
&lt;p&gt;A sample command invocation looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cm-burn -name  red[5-7] \
        --key ~/.ssh/id_rsa.pub \
        -ips 192.168.1.[5-7] \
        -image 2018-06-27-raspbian-stretch
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command creates 3 SD cards where the hostnames &lt;code&gt;red5&lt;/code&gt;, &lt;code&gt;red6&lt;/code&gt;, &lt;code&gt;red 7&lt;/code&gt;
with the network addresses &lt;code&gt;192.168.1.5&lt;/code&gt;, &lt;code&gt;192.168.1.6&lt;/code&gt;,
and &lt;code&gt;192.168.1.7&lt;/code&gt;. The public key is added to the authorized_keys file
of the pi user.  The password login is automatically disabled and only
the ssh key authentication is enabled.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudmesh-community/cm-burn&#34;&gt;https://github.com/cloudmesh-community/cm-burn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Scientific Impact</title>
      <link>/project/impact/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 -0400</pubDate>
      <guid>/project/impact/</guid>
      <description>&lt;p&gt;In scientific impact metrics we attempt to evaluate the work conducted
by researcher teams or organizations and compare how they perform to
other groups.&lt;/p&gt;
&lt;p&gt;In our effort we use the bibliometrics approach to evaluate the
scientific impact of XSEDE. By utilizing publication data from various
sources, e.g., ISI Web of Science and Microsoft Academic Graph, we
calculate the impact metrics of XSEDE publications and show how they
compare with non-XSEDE publication from the same field of study, or
non-organization peers from the same journal issue. To perform this
analysis we need to identify the bibliographic dataset and clean and
currate them.&lt;/p&gt;
&lt;p&gt;We then introduce the metrics we used for evaluation and comparison, and
the methods used to calculate them. Detailed analysis results of Field
Weighted Citation Impact (FWCI) and the peers comparison will be
presented and discussed. We also explain how the same approaches could
be used to evaluate publications from a similar organization or
institute, to demonstrate the general applicability of the present
evaluation approach providing impact on organizations and groups.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;/publication/las-18-impact&#34;&gt;Evaluating the Scientific Impact of XSEDE Fugang Wang, Gregor von
Laszewski, Timothy Whitson, Geoffrey C Fox, Thomas R Furlani,
Robert L DeLeon, Steven M Gallo Jan 1,
2018&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;/publication/las-15-impact-ncar&#34;&gt;Peer Comparison of XSEDE and NCAR Publication Data G. von Laszewski,
F. Wang, G. C. Fox, D. L. Hart, T. R. Furlani, R. L. DeLeon, S. M.
Gallo Sep 1, 2015&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;/publication/las-14-impact&#34;&gt;Towards a Scientific Impact Measuring Framework for Large Computing
Facilities - a Case Study on XSEDE Fugang Wang, Gregor von Laszewski,
Geoffrey C. Fox, Thomas R. Furlani, Robert L. DeLeon, Steven M. Gallo
Jan 1, 2014&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;/publication/las-15-tas&#34;&gt;TAS View of XSEDE Users and Usage Robert L. DeLeon, Thomas R.
Furlani, Steven M. Gallo, Joseph P. White, Matthew D. Jones, Abani
Patra, Martins Innus, Thomas Yearke, Jeffrey T. Palmer, Jeanette M.
Sperhac, Ryan Rathsam, Nikolay Simakov, Gregor von Laszewski, Fugang
Wang Jan 1, 2015&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
