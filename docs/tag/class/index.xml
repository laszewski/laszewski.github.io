<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>class | Gregor von Laszewski</title>
    <link>/tag/class/</link>
      <atom:link href="/tag/class/index.xml" rel="self" type="application/rss+xml" />
    <description>class</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 01 Aug 2021 01:33:02 -0500</lastBuildDate>
    <image>
      <url>/images/icon_hufdd866d90d76849587aac6fbf27da1ac_464_512x512_fill_lanczos_center_3.png</url>
      <title>class</title>
      <link>/tag/class/</link>
    </image>
    
    <item>
      <title>Kubernetes Raspberry Pi Cluster for Data Science (largely completed)</title>
      <link>/post/project-pi-kubernetes/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-pi-kubernetes/</guid>
      <description>&lt;p&gt;Be part of a team to create a kubernetes cluster on a Raspberry pi.&lt;/p&gt;
&lt;p&gt;At 
&lt;a href=&#34;http://piplanet.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster with a convenient burning approach of SD Cards. The Pis will
receive preconfigured SD Cards that allow starting a cluster when the
Pis are switched on. You will be able to communicate from the COmputer
on which you burned the Pis and each of the PIs. You will also be able
to communicate between the PIs.&lt;/p&gt;
&lt;p&gt;Now that the basic cluster is set up, we like to install a distributed
quieing system for the cluster based on Kubernetes. Kubernetes is a very popular
farmework for container clusters.&lt;/p&gt;
&lt;p&gt;We like to implement a simple one line deployment comamndline tool supported by an API that deploys kubernetes on a list of hosts specified by the ip adress. This includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to have financial resources to buy yourself the material for creating a PI cluster. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at 
&lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you also have a Laptop or desktop to which you like to connect the PI, make sure it can run docker (Windows Home will not work). However, one of the Raspberry PIs will do.&lt;/li&gt;
&lt;li&gt;Significant Python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;We like you to explore the deployment both on RaspberryOS and on Ubuntu.&lt;/p&gt;
&lt;p&gt;Please be aware that the goal is not to replicate tutorials from the Web that require input by hand or repeated installs on the various PIs. Instead, we like to have  a single command such as&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cms pi cluster deploy kubernetes --hosts &amp;quot;red,red[01-red02]&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where simple options such as the hostnames in the pi cluster are used.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all projects on the internet that do something similar&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Explore the old code and identify what is wrong with it (just by looking at it)&lt;/li&gt;
&lt;li&gt;Identify a base method. This can use existing DevOps approaches such as ansible, cheff, snapcraft (ubuntu), or cloudmesh parallel runtime methods. However, the use will be hidden through an API and command line tool. Evaluate the llnl simple deployment method which can be used. Also exploer if there is a snap available.&lt;/li&gt;
&lt;li&gt;From the command line, derive some API interface and use FastAPI to
implement it.&lt;/li&gt;
&lt;li&gt;Identify a mechanism on how to deal with the security.&lt;/li&gt;
&lt;li&gt;Before you start implementing, describe and showcase a couple of commands on how to use it.&lt;/li&gt;
&lt;li&gt;Your deployment should target Ubuntu on the PI as well as RaspberryOS. If there is only time for one, pick one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Map-Reduce Raspberry Pi Cluster as a Local Cloud</title>
      <link>/post/project-pi-spark/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-pi-spark/</guid>
      <description>&lt;p&gt;Be part of a team to create a spark cluster on a Raspberry pi.&lt;/p&gt;
&lt;p&gt;At 
&lt;a href=&#34;http://piplanet.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster with a convenient burning approach of SD Cards. The Pis will
receive preconfigured SD Cards that allow starting a cluster when the
Pis are switched on. You will be able to communicate from the COmputer
on which you burned the Pis and each of the PIs. You will also be able
to communicate between the PIs.&lt;/p&gt;
&lt;p&gt;Now that the basic cluster is set up, we like to install a data science
framework such as Spark and Hadoop and showcase its use. Although
previously we had some effort implementing Hadoop and Spark, these
efforts were not production-ready and had significant gaps in its
implementation.&lt;/p&gt;
&lt;p&gt;We like to reimplement these projects and significantly improve them by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to have financial resources to buy yourself the material for creating a PI cluster. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at 
&lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you also have a Laptop or desktop to which you like to connect the PI, make sure it can run docker (Windows Home will not work). However, one of the Raspberry PIs will do.&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;There is a great overlap between installing Hadoop and Spark on a
cluster. We recommend that the team slits up the work but collaborates
intensely to create a uniform solution.&lt;/p&gt;
&lt;p&gt;Please be aware that the goal is not to replicate tutorials from the Web that require input by hand or repeated installs on the various PIs. Instead, we like to have  a single command such as&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cms pi cluster deploy spark --hosts &amp;quot;red,red[01-red02]&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where simple options such as the hostnames in the pi cluster are used.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all projects on the internet that do something similar&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Explore the old code and identify what is wrong with it (just by looking at it)&lt;/li&gt;
&lt;li&gt;Identify a new deployment method. This can use existing DevOps approaches such as ansible, cheff, snapcraft (ubuntu), or cloudmesh parallel runtime methods. However, the use will be hidden through an API and command line tool.&lt;/li&gt;
&lt;li&gt;From the command line, derive some API interface and use FastAPI to
implement it.&lt;/li&gt;
&lt;li&gt;Identify a mechanism on how to deal with the security.&lt;/li&gt;
&lt;li&gt;Before you start implementing, describe and showcase a couple of commands on how to use it.&lt;/li&gt;
&lt;li&gt;Your deployment should target Ubuntu on the PI as well as RaspberryOS. If there is only time for one, pick one.
Note that Hadoop or Spark may also be able to be installed on the PIs with snapcraft. Discuss how this solution could be used for the implementation of this project.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Cloud NLP Service</title>
      <link>/post/project-nlp/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-nlp/</guid>
      <description>&lt;p&gt;Be part of a team to create a multicloud natural language processing service.&lt;/p&gt;
&lt;p&gt;Your goal will be to develop an API, secure REST, and command line
tool that easily interfaces with natural language services of multiple
cloud providers.  Your integrated service will utilize all or one
of them to achieve a task related to NLP analysis.&lt;/p&gt;
&lt;p&gt;This is especially useful for data scientists that may want to access
multiple cloud providers and eliminate vendor lock-in or to access
services that other providers do not offer.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;p&gt;You will be developing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A comparison of NLP cloud services.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aylien&lt;/li&gt;
&lt;li&gt;Text Summarization&lt;/li&gt;
&lt;li&gt;Twinword Text Analysis&lt;/li&gt;
&lt;li&gt;IBM Watson Alchemy&lt;/li&gt;
&lt;li&gt;RxNLP&lt;/li&gt;
&lt;li&gt;Linguakit&lt;/li&gt;
&lt;li&gt;Geneea Interpretor NLP&lt;/li&gt;
&lt;li&gt;MLP CLoud&lt;/li&gt;
&lt;li&gt;Natural Language AI 
&lt;a href=&#34;https://cloud.google.com/natural-language&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloud.google.com/natural-language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CLoud Factory NLP 
&lt;a href=&#34;https://www.cloudfactory.com/services/nlp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cloudfactory.com/services/nlp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MonkeyLearn&lt;/li&gt;
&lt;li&gt;MeaningCloud&lt;/li&gt;
&lt;li&gt;Lexalytics&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;li&gt;There could be many more&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Distinguish them by characteristics, create a table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A command line interface to the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Rest API that calls out other services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Python API that wraps several services into a convenient library&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A manual describing the functionality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Put everything in a container so it can be run on Linux, Mac and
Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a convenient command line tool that allows starting the service, interacting with it, and making this really easy to use. THe
command line will hide the docker commands while providing human
readable abbreviations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver unit tests with pytests&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver a high-quality report including benchmarks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrate authentication to the cloud providers and to the REST service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Yaml for the configuration of the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the development in a container using 20.04. We will create a DOckerfile&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code will be developed in GitHub at cloudmesh-nlp, which will be set up by Gregor&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A computer on which you can run docker (Windows Home will not work)&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;Showcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;Be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;start with a Survey&lt;/li&gt;
&lt;li&gt;design the command line interface first as that may be the easiest and will showcase how to design the API&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Cloud Virtual Directory</title>
      <link>/post/project-vdir/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-vdir/</guid>
      <description>&lt;h2 id=&#34;virtual-directory&#34;&gt;Virtual directory&lt;/h2&gt;
&lt;p&gt;Be part of a team to create a multicloud virtual directory.&lt;/p&gt;
&lt;p&gt;Your goal will be to to develop an API, secure REST, and commandline
tool that easily interfaces with storage servoces such as file based
services and object stores osted on the internet to create a virtual
directory that accesses the files by directory name and filename.&lt;/p&gt;
&lt;p&gt;This is espaciellyusefull for data scientists that may want to strore
their data on multiple cloud providers and eliminate vendor lockin.&lt;/p&gt;
&lt;p&gt;It could also be useful for geographically distributed files that
allow services to utilize them based on speed of access or the
physical location.&lt;/p&gt;
&lt;p&gt;In a previous version of cloudmesh we have developed an integrated
approach for compute and storage services. However in this project we
like to explore some new features and separate the implementation form
the main cloudmesh.&lt;/p&gt;
&lt;p&gt;In the past cloudmesh has conducted two such projects&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/manual/storage.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/manual/storage.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://cloudmesh.github.io/cloudmesh-manual/manual/vdir.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloudmesh.github.io/cloudmesh-manual/manual/vdir.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We like to reimplement thise projects and significantly improve them by&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Adding more providers&lt;/li&gt;
&lt;li&gt;Adding a secure rest service for acceassing the virdual directory&lt;/li&gt;
&lt;li&gt;Creating a slick user interface to brows the virdula directry&lt;/li&gt;
&lt;li&gt;Put everything in a container so it can be run on Linux, MAc and
Windows.&lt;/li&gt;
&lt;li&gt;Dreate a convenient commandline tool that allows starting of the
service, interacting with it and make this real easy to use. THe
commandline will hide the docker commands while providing human
readable abbreviations.&lt;/li&gt;
&lt;li&gt;Deliver unit tests with pytests&lt;/li&gt;
&lt;li&gt;Deliver a high quality report including benchmarks&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A computer on which you can run docker (Windows Home will not work)&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgable with github (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;One of the two projects used a single file taansfer logic, while the
other used a parallel file transfer option. It will be up to you to decide how to proceed.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In the first week collect all clouds and services that could be
used in this project. THis work should be split between the
teammembers and a detailed analysis including features including
authentication and security must be conducted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You will need to evaluate the previous efforts and decide how to proceed.&lt;/p&gt;
&lt;p&gt;One way is to select several cloudas and define an abstraction that
has to be implemented for each cloud. this can be actually bes
demonstrated by an example commandline such as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vdir mkdir DIR
vdir cd [DIR]
vdir ls [DIR]
vdir add [FILEENDPOINT] [DIR_AND_NAME]
vdir delete [DIR_OR_NAME]
vdir status [DIR_OR_NAME]
vdir get NAME DESTINATION
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the commandline derive some API interface and use FastAPI to
implement it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identify a mechnism on how to deal with the security. AN encrypted
file with your cloud credentials will be sufficient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the limitations of the previous vdir project is that the
fill add function is done only on a single file but not on a
recursive file tree. Naturally we need to upload, single files,
multiple files and filetrees, similar to the unix command rsync.
This has been implemented in the storage command&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;storage [--storage=SERVICE] [--parallel=N] create dir DIRECTORY
storage [--storage=SERVICE] [--parallel=N] get SOURCE DESTINATION [--recursive]
storage [--storage=SERVICE] [--parallel=N] put SOURCE DESTINATION [--recursive]
storage [--storage=SERVICE] [--parallel=N] list [SOURCE] [--recursive] [--output=OUTPUT]
storage [--storage=SERVICE] [--parallel=N] delete SOURCE
storage [--storage=SERVICE] search  DIRECTORY FILENAME [--recursive] [--output=OUTPUT]
storage [--storage=SERVICE] sync SOURCE DESTINATION [--name=NAME] [--async]
storage [--storage=SERVICE] sync status [--name=NAME]
storage config list [--output=OUTPUT]
storage [--parallel=N] copy SOURCE DESTINATION [--recursive]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We like naturally to (re-)implement the storage command.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before you start implementing describe and showcase a couple of commands on how to used it.
Our command will be called cdir (for cloudmesh dir so it is easy to distinguish)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure that you provide a comprehensive list of potentail cloud
storage ervices. Distinguish between for pay and free services.&lt;/p&gt;
&lt;p&gt;The list could include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWS Drive&lt;/li&gt;
&lt;li&gt;AWS Object Store&lt;/li&gt;
&lt;li&gt;An ssh accesible computer&lt;/li&gt;
&lt;li&gt;Azure&lt;/li&gt;
&lt;li&gt;Box&lt;/li&gt;
&lt;li&gt;Degoo&lt;/li&gt;
&lt;li&gt;Dropbox&lt;/li&gt;
&lt;li&gt;Google Drive&lt;/li&gt;
&lt;li&gt;Google docs/drive&lt;/li&gt;
&lt;li&gt;Icedrive&lt;/li&gt;
&lt;li&gt;MEGA&lt;/li&gt;
&lt;li&gt;MediaFire&lt;/li&gt;
&lt;li&gt;MediaFire&lt;/li&gt;
&lt;li&gt;Mega&lt;/li&gt;
&lt;li&gt;Nextcloud&lt;/li&gt;
&lt;li&gt;OneCloud&lt;/li&gt;
&lt;li&gt;OneDrive&lt;/li&gt;
&lt;li&gt;Oracle&lt;/li&gt;
&lt;li&gt;Sync.com&lt;/li&gt;
&lt;li&gt;WebDAV&lt;/li&gt;
&lt;li&gt;Your local computer&lt;/li&gt;
&lt;li&gt;iCloud&lt;/li&gt;
&lt;li&gt;iDrive&lt;/li&gt;
&lt;li&gt;idrive&lt;/li&gt;
&lt;li&gt;pCloud&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Are there companies or services that already offer this?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When defining the REST API we like to be able to use a dirname and
a basename within the api similar to python. to deal with provider
specific filenames we do have two urls for a file.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{provider}/{dirname}/{basename}&lt;/code&gt;
&lt;code&gt;{dirname}/{basename}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The first allows duplication of files between services, the latter
defines a &amp;ldquo;preferred&amp;rdquo; url based on some criteria. It could be that
we just set the preferred provider for the file.&lt;/p&gt;
&lt;p&gt;Note that the introduction of duplicating files is new&lt;/p&gt;
&lt;p&gt;It also requires an update so that the file with the newest
timestamp gets updated on all registered serrvices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Please note that the project addresses the ability to integrate
object store and regular file system based storage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not all implementation must be done on Ubuntu20.04 which is run
via a container. The graphical component is done wia moder Web
view technologies. Wile all other implementation can be done in
python, the GUI can also be implemented in JavaScript. The service
accessing other services must be properly protected which is easy
as we assum it runs on localhost and we can appropriately secure
it with common solutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of qeuestions, pleas econtact Gregor at&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Cloud X Service</title>
      <link>/post/project-multicloud-x/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-multicloud-x/</guid>
      <description>&lt;p&gt;Be part of a team to create a multicloud access to a service offered
by multiple cloud or cloud service providers.&lt;/p&gt;
&lt;p&gt;Your goal will be to develop an API, secure REST, and command line
tool that easily interfaces with a service framework or tool offered by multiple
cloud providers.  Your integrated service will utilize all or one
of them to achieve a task related to an analysis conducted by X.&lt;/p&gt;
&lt;p&gt;This is especially useful for data scientists that may want to access
multiple cloud providers and eliminate vendor lock-in or to access
services that other providers do not offer.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;p&gt;You will be developing&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A comparison of X on various cloud or cloud service providers.
Distinguish them by chracteroistics, create a table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A command line interface to the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Rest API that calls out other services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Python API that wraps several services into a convenient library&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A manual describing the functionality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Put everything in a container so it can be run on Linux, Mac and
Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a convenient command line tool that allows starting the service, interacting with it, and making this easy to use. The command line will hide the docker commands while providing human
readable abbreviations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver unit tests with pytests&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deliver a high-quality report including benchmarks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrate authentication to the cloud providers and to the REST service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Yaml for the configuration of the service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do the development in a container using 20.04. We will create a DOckerfile&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code will be developed in GitHub at cloudmesh, which will be set up by Gregor&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A computer on which you can run docker (Windows Home will not work)&lt;/li&gt;
&lt;li&gt;Significant python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;Showcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;Be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;start with a Survey&lt;/li&gt;
&lt;li&gt;design the command line interface first as that may be the easiest and will showcase how to design the API&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In case of questions, please contact Gregor at&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quality teaching</title>
      <link>/post/class/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/class/</guid>
      <description>&lt;p&gt;You may like some of the quotes from students:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Best class I ever took at IU.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Thanks to you I got a job at a top rated company.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;My company has adopted lessons learned from your class.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The level of support and expertise the professor provides is outstanding and unmatched within IU.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All classes can be taken by graduate and undergraduate students.
Undergraduate students may need permission from me for some classes. In
the past we had dedicated undergraduates participatintg in the class
that were in the top 5% of the class.&lt;/p&gt;
&lt;h2 id=&#34;independent-study&#34;&gt;Independent Study&lt;/h2&gt;
&lt;p&gt;Independent Studies are any time available for graduate and
undergraduate students.  Contact Gregor at 
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;.
Independent studies will work on programming projects, advanced cloud
topics, or conference/workshop papers.&lt;/p&gt;
&lt;h2 id=&#34;undergraduate-reu&#34;&gt;Undergraduate REU&lt;/h2&gt;
&lt;p&gt;This is a unique paid opportunity. It is available for US Citizens or
Permanent Residents. Strong knowledge in Linux and Python is required.
20 hour per week commitment is required. Please inqure with Gregor to
see if you are qualified.&lt;/p&gt;
&lt;p&gt;I have immediate REU positions available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slurm Raspberry Pi Cluster for Data Science</title>
      <link>/post/project-pi-slurm/</link>
      <pubDate>Sun, 01 Aug 2021 01:33:02 -0500</pubDate>
      <guid>/post/project-pi-slurm/</guid>
      <description>&lt;p&gt;Be part of a team to create a slurm cluster on a Raspberry pi.&lt;/p&gt;
&lt;p&gt;At 
&lt;a href=&#34;http://piplanet.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://piplanet.org&lt;/a&gt; we discuss setting up a Raspberry Pi
cluster with a convenient burning approach of SD Cards. The Pis will
receive preconfigured SD Cards that allow starting a cluster when the
Pis are switched on. You will be able to communicate from the COmputer
on which you burned the Pis and each of the PIs. You will also be able
to communicate between the PIs.&lt;/p&gt;
&lt;p&gt;Now that the basic cluster is set up, we like to install a distributed
quieing system for the cluster based on Slurm. Slurm is a very popular
farmework for HPC computers, but also allows scheduling of long
running jobs for data science.&lt;/p&gt;
&lt;p&gt;We like to implement a simple one line deployment comamndline tool supported by an API that deploys slurm on a list of hosts specified by the ip adress. This includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a proper method to deploy the frameworks on the cluster
given the IP&amp;rsquo;s or the name of the machines.&lt;/li&gt;
&lt;li&gt;Provide a mechanism to verify if the deployment was successful&lt;/li&gt;
&lt;li&gt;Provide a set of unit tests using &lt;code&gt;pytest&lt;/code&gt; to execute some basic use cases.&lt;/li&gt;
&lt;li&gt;Develop a python API that supports the deployment while exposing it through a command line&lt;/li&gt;
&lt;li&gt;Explore the development of a REST API that facilitates the deployment reusing the developed Python API.&lt;/li&gt;
&lt;li&gt;Develop a manual&lt;/li&gt;
&lt;li&gt;Develop a high-quality report with benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;p&gt;In order for you to participate in this project, you will need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You need to have financial resources to buy yourself the material for creating a PI cluster. You will need at least 3 Pis with 8GB memory. Each of them costs $75, but you also need a power supply costing $35, power cables, network cables, and at least one HDMI cable suitable to connect an HDMI monitor to a PI. More details about parts can be found at 
&lt;a href=&#34;https://cloudmesh.github.io/pi/docs/hardware/parts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloudmesh.github.io/pi/docs/hardware/parts/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you also have a Laptop or desktop to which you like to connect the PI, make sure it can run docker (Windows Home will not work). However, one of the Raspberry PIs will do.&lt;/li&gt;
&lt;li&gt;Significant Python knowledge&lt;/li&gt;
&lt;li&gt;Be highly motivated&lt;/li&gt;
&lt;li&gt;Be willing to have meetings on this project once or twice a week&lt;/li&gt;
&lt;li&gt;SHowcase significant progress over the lifetime of the project.&lt;/li&gt;
&lt;li&gt;be knowledgeable with GitHub (a repository will be provided to which
Dr. von Laszewski will contribute)&lt;/li&gt;
&lt;li&gt;Conduct task management in GitHub (Gregor will explain)&lt;/li&gt;
&lt;li&gt;Be honest and not hiding problems or implementation bugs.&lt;/li&gt;
&lt;li&gt;You must be able to do a videoconference and be able to share your screen (I typically use google meet or zoom).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;options&#34;&gt;Options&lt;/h2&gt;
&lt;p&gt;We like you to explore the deployment both on RaspberryOS and on Ubuntu.&lt;/p&gt;
&lt;p&gt;Please be aware that the goal is not to replicate tutorials from the Web that require input by hand or repeated installs on the various PIs. Instead, we like to have  a single command such as&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cms pi cluster deploy slurm --hosts &amp;quot;red,red[01-red02]&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where simple options such as the hostnames in the pi cluster are used.&lt;/p&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In the first week, collect a list of all projects on the internet that do something similar&lt;/li&gt;
&lt;li&gt;Study the GitHub repositories from cloudmesh-pi-burn and cloudmesh-pi-cluster and cloudmesh-common intensely. Explore the old code and identify what is wrong with it (just by looking at it)&lt;/li&gt;
&lt;li&gt;Identify a base method. This can use existing DevOps approaches such as ansible, cheff, snapcraft (ubuntu), or cloudmesh parallel runtime methods. However, the use will be hidden through an API and command line tool. Evaluate the llnl simple deployment method which can be used. Also exploer if there is a snap available.&lt;/li&gt;
&lt;li&gt;From the command line, derive some API interface and use FastAPI to
implement it.&lt;/li&gt;
&lt;li&gt;Identify a mechanism on how to deal with the security.&lt;/li&gt;
&lt;li&gt;Before you start implementing, describe and showcase a couple of commands on how to use it.&lt;/li&gt;
&lt;li&gt;Your deployment should target Ubuntu on the PI as well as RaspberryOS. If there is only time for one, pick one.&lt;/li&gt;
&lt;li&gt;Please be reminded that you also need to set up NFS on the nodes. There are many examples provided how to do that. You will be using the USB port to include a disk. It may require an external USB drive or a simple USB stick. Make sure they are USB 3 compatible.
In case of questions, please contact Gregor at&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
